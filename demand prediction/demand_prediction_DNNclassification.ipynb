{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376328\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/demandClassifier15', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024597EC3F28>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-06-22:16:26\n",
      "INFO:tensorflow:Restoring parameters from /tmp/demandClassifier15\\model.ckpt-851\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-06-22:16:28\n",
      "INFO:tensorflow:Saving dict for global step 851: accuracy = 0.888201, average_loss = 0.27995, global_step = 851, loss = 532100.0\n",
      "INFO:tensorflow:Restoring parameters from /tmp/demandClassifier15\\model.ckpt-851\n",
      "New Samples, Class Predictions:    [array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'5'], dtype=object), array([b'0'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'5'], dtype=object), array([b'6'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'4'], dtype=object), array([b'1'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'2'], dtype=object), array([b'3'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object), array([b'0'], dtype=object)]\n",
      "\n",
      "         Actual_Count Predicted_Count\n",
      "975308              0          [b'0']\n",
      "972823              0          [b'0']\n",
      "243929              0          [b'0']\n",
      "264406              1          [b'1']\n",
      "195657              0          [b'0']\n",
      "886463              0          [b'0']\n",
      "1173179             0          [b'0']\n",
      "1067876             1          [b'1']\n",
      "92198               0          [b'0']\n",
      "259671              1          [b'1']\n",
      "1293448             0          [b'0']\n",
      "733137              0          [b'0']\n",
      "1126324             0          [b'0']\n",
      "827959              0          [b'0']\n",
      "459503              0          [b'0']\n",
      "535220              0          [b'0']\n",
      "321115              0          [b'0']\n",
      "190479              2          [b'2']\n",
      "1137668             1          [b'1']\n",
      "447722              0          [b'0']\n",
      "1055241             0          [b'0']\n",
      "808194              0          [b'0']\n",
      "842156              0          [b'0']\n",
      "1110114             3          [b'3']\n",
      "333934              8          [b'6']\n",
      "1228095             2          [b'2']\n",
      "1268422             0          [b'0']\n",
      "1208530             0          [b'0']\n",
      "1091351             0          [b'0']\n",
      "918496              0          [b'0']\n",
      "...               ...             ...\n",
      "45808               0          [b'0']\n",
      "452745              0          [b'0']\n",
      "1366556             0          [b'0']\n",
      "404672              0          [b'0']\n",
      "1283327             2          [b'2']\n",
      "308541              0          [b'0']\n",
      "348770              0          [b'0']\n",
      "851679              1          [b'1']\n",
      "70605               0          [b'0']\n",
      "1045019             0          [b'0']\n",
      "912873              0          [b'0']\n",
      "1280292             0          [b'0']\n",
      "163686              0          [b'0']\n",
      "658635              0          [b'0']\n",
      "673033              1          [b'1']\n",
      "47930               0          [b'0']\n",
      "1201457             0          [b'0']\n",
      "507860              0          [b'0']\n",
      "888999              2          [b'2']\n",
      "518946              0          [b'0']\n",
      "25254               0          [b'0']\n",
      "934149              0          [b'0']\n",
      "581187              0          [b'0']\n",
      "487069              0          [b'0']\n",
      "630441              0          [b'0']\n",
      "643604              0          [b'0']\n",
      "976807              0          [b'0']\n",
      "705616              0          [b'0']\n",
      "457884              0          [b'0']\n",
      "1074066             5          [b'5']\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#This script reads in cleaned and merged data and performs regressions to predict number of bikes\n",
    "#departing a bike station given the following parameters:\n",
    "#Time bucket, station latitude and longitude, hourly temperature, precipitation, and whether the day is a holiday/weekend or not. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#generate tensorflow input function for gradient descent. \n",
    "def get_input_fn(data_set, num_epochs,shuffle):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x = pd.DataFrame({k: data_set[k].values for k in FEATURES_WEIGHT}),\n",
    "      y = pd.Series(data_set[LABEL].values),\n",
    "      num_epochs=num_epochs,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=data_set.shape[0])\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    COLUMNS = ['Start_Time','Start_Station_Latitude','Start_Station_Longitude','Count','Precipitation','Temperature']\n",
    "    FEATURES = ['Start_Time','Start_Station_Latitude','Start_Station_Longitude','Precipitation','Temperature']\n",
    "    #This is to assign more weight to under-represented classes to combat the problem of imbalanced data. \n",
    "    FEATURES_WEIGHT = ['Start_Time','Start_Station_Latitude','Start_Station_Longitude','Precipitation','Temperature','Weight']\n",
    "    LABEL = 'Count'   \n",
    "                                      \n",
    "    #read data                             \n",
    "    data = pd.read_csv('demand_prediction_data.csv')\n",
    "    \n",
    "    #Assign the number of bikes into intervals of length 4, in order to perform classification. \n",
    "    #In this case, we have 9 classes, with last class anything larger than 31.\n",
    "    bins = [-1,4,8,12,16,20,24,28,32,1000]\n",
    "    data['Count'] = pd.cut(data['Count'],bins=bins,labels=False).astype(int)\n",
    "    \n",
    "    #normalize feature and label vectors\n",
    "    for k in FEATURES:\n",
    "        data[k] = (data[k] - data[k].mean()) / (data[k].max() - data[k].min())\n",
    "        \n",
    "    #Add weight inversely proportional to the frequency of classes. \n",
    "    weight = pd.DataFrame()\n",
    "    weight['Count'] = range(len(bins)-1)\n",
    "    frequency = np.asarray(data.groupby('Count').size())\n",
    "    numerical_weight = 1/(frequency / np.linalg.norm(frequency))\n",
    "    weight['Weight'] = numerical_weight\n",
    "    \n",
    "    cols = ['Count']\n",
    "    data = data.join(weight.set_index(cols), on=cols)\n",
    "    \n",
    "    #randomly split data set into 70% training set, 20% validation set, and 10% test set.\n",
    "    length = len(data['Count'])\n",
    "    print(length)\n",
    "    selection = np.random.rand(length)\n",
    "    training = (selection < 0.7)\n",
    "    training_set = data[training]\n",
    "    valid_test = data[~training]\n",
    "    \n",
    "    length = len(valid_test['Count'])\n",
    "    selection = np.random.rand(length)\n",
    "    valid = (selection < 0.66)\n",
    "    validation_set = valid_test[valid]\n",
    "    test_set = valid_test[~valid]\n",
    "    \n",
    "    #define feature columns \n",
    "    feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES_WEIGHT]\n",
    "    \n",
    "    #Evaluate on validation set every 10 epochs, and train for 1000 epochs. \n",
    "    for i in range(1):\n",
    "        \n",
    "        #Implement classification on Tensorflow.\n",
    "        \n",
    "        classifier = tf.estimator.DNNClassifier(feature_columns=feature_cols, hidden_units=[64,32,64],                     \n",
    "                          #optimizer=tf.train.FtrlOptimizer(\n",
    "                          #learning_rate=0.1,\n",
    "                          #l1_regularization_strength=10,\n",
    "                          #l2_regularization_strength=10))\n",
    "                          #optimizer=tf.train.AdamOptimizer(\n",
    "                          #learning_rate=0.001,\n",
    "                          #beta1=0.9,\n",
    "                          #beta2=0.999,  \n",
    "                          #epsilon=1e-8),\n",
    "                          n_classes=9,                 \n",
    "                          model_dir=\"/tmp/demandClassifier15\",\n",
    "                          weight_column=\"Weight\")\n",
    "        \n",
    "        #classifier.train(input_fn=get_input_fn(training_set,num_epochs=10, shuffle=True), steps=10)\n",
    "        \n",
    "        ev = classifier.evaluate(\n",
    "        input_fn=get_input_fn(validation_set, num_epochs=1, shuffle=False))\n",
    "        \n",
    "    #To generate examples of predictions, randomly select 100 rows from the test set. \n",
    "    snip = test_set.sample(100)\n",
    "    pred = list(classifier.predict(\n",
    "    input_fn=get_input_fn(snip, num_epochs=1, shuffle=False)))\n",
    "    predictions = [p[\"classes\"] for p in pred]\n",
    "    comparison = pd.DataFrame()\n",
    "    comparison['Actual_Count']=snip['Count'] \n",
    "    comparison['Predicted_Count']=predictions\n",
    "    print(comparison.sample(frac=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
